{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unity ML Agents\n",
    "## Proximal Policy Optimization (PPO)\n",
    "Contains an implementation of PPO as described [here](https://arxiv.org/abs/1707.06347)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'tensorflow' from 'd:\\\\programdata\\\\anaconda3\\\\envs\\\\tensorflow-avx\\\\lib\\\\site-packages\\\\tensorflow\\\\__init__.py'>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf)\n",
    "\n",
    "from ppo.history import *\n",
    "from ppo.models import *\n",
    "from ppo.trainer import Trainer\n",
    "from unityagents import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### General parameters\n",
    "max_steps = 5e5 # Set maximum number of steps to run environment.\n",
    "run_path = \"ppo\" # The sub-directory name for model and summary statistics\n",
    "load_model = False # Whether to load a saved model.\n",
    "train_model = True # Whether to train the model.\n",
    "summary_freq = 2500 # Frequency at which to save training statistics.\n",
    "save_freq = 50000 # Frequency at which to save model.\n",
    "env_name = \"3dball\" # Name of the training environment file.\n",
    "curriculum_file = None\n",
    "\n",
    "### Algorithm-specific parameters for tuning\n",
    "gamma = 0.99 # Reward discount rate.\n",
    "lambd = 0.95 # Lambda parameter for GAE.\n",
    "time_horizon = 2048 # How many steps to collect per agent before adding to buffer.\n",
    "beta = 1e-3 # Strength of entropy regularization\n",
    "num_epoch = 12 # Number of gradient descent steps per batch of experiences.\n",
    "num_layers = 3 # Number of hidden layers between state/observation encoding and value/policy layers.\n",
    "epsilon = 0.2 # Acceptable threshold around ratio of old and new policy probabilities.\n",
    "buffer_size = 4096 # How large the experience buffer should be before gradient descent.\n",
    "learning_rate = 1.5e-4 # Model learning rate.\n",
    "hidden_units = 80 # Number of units in hidden layer.\n",
    "batch_size = 1024 # How many experiences per gradient descent update step.\n",
    "normalize = False\n",
    "\n",
    "### Logging dictionary for hyperparameters\n",
    "hyperparameter_dict = {'max_steps':max_steps, 'run_path':run_path, 'env_name':env_name,\n",
    "    'curriculum_file':curriculum_file, 'gamma':gamma, 'lambd':lambd, 'time_horizon':time_horizon,\n",
    "    'beta':beta, 'num_epoch':num_epoch, 'epsilon':epsilon, 'buffe_size':buffer_size,\n",
    "    'leaning_rate':learning_rate, 'hidden_units':hidden_units, 'batch_size':batch_size}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Ball3DAcademy' started successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unity Academy name: Ball3DAcademy\n",
      "        Number of brains: 1\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: Ball3DBrain\n",
      "        Number of observations (per agent): 0\n",
      "        State space type: continuous\n",
      "        State space size (per agent): 18\n",
      "        Action space type: continuous\n",
      "        Action space size (per agent): 6\n",
      "        Memory space size (per agent): 0\n",
      "        Action descriptions: rot z, rot x, rot y, move x, move y, move z\n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=env_name, curriculum=curriculum_file)\n",
    "print(str(env))\n",
    "brain_name = env.external_brain_names[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Agent(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 2500. Mean Reward: -0.5222308470265059. Std of Reward: 0.4651253859278168.\n",
      "Step: 5000. Mean Reward: -0.09790442811376997. Std of Reward: 0.9741602204579446.\n",
      "Step: 7500. Mean Reward: 0.6996831366959544. Std of Reward: 2.219883808666055.\n",
      "Step: 10000. Mean Reward: 1.8355062384879675. Std of Reward: 3.7586725155475955.\n",
      "Step: 12500. Mean Reward: 3.133852128480086. Std of Reward: 5.020025479647246.\n",
      "Step: 15000. Mean Reward: 6.60581354016235. Std of Reward: 9.107321939188475.\n",
      "Step: 17500. Mean Reward: 6.481737854255683. Std of Reward: 8.755170151376799.\n",
      "Step: 20000. Mean Reward: 10.306838699883842. Std of Reward: 13.088978536853588.\n",
      "Step: 22500. Mean Reward: 11.629050286699995. Std of Reward: 14.275091108002103.\n",
      "Step: 25000. Mean Reward: 16.06578516498151. Std of Reward: 18.133609811254217.\n",
      "Step: 27500. Mean Reward: 19.096468722906735. Std of Reward: 17.191318204471877.\n",
      "Step: 30000. Mean Reward: 10.743319416717364. Std of Reward: 15.580513216447326.\n",
      "Step: 32500. Mean Reward: 7.938519207607584. Std of Reward: 12.385768045135357.\n",
      "Step: 35000. Mean Reward: 16.555939642172948. Std of Reward: 18.20178685155854.\n",
      "Step: 37500. Mean Reward: 21.29011485538285. Std of Reward: 21.33976657348676.\n",
      "Step: 40000. Mean Reward: 17.13246098509774. Std of Reward: 20.945933006358675.\n",
      "Step: 42500. Mean Reward: 19.32835996498223. Std of Reward: 19.278566026128885.\n",
      "Step: 45000. Mean Reward: 23.120639960004084. Std of Reward: 21.287319004599077.\n",
      "Step: 47500. Mean Reward: 15.063177595643706. Std of Reward: 19.7686116956961.\n",
      "Step: 50000. Mean Reward: 21.053364621102382. Std of Reward: 22.20886584772135.\n",
      "Saved Model\n",
      "Step: 52500. Mean Reward: 15.149084079701314. Std of Reward: 18.027189690773763.\n",
      "Step: 55000. Mean Reward: 10.233713697354215. Std of Reward: 15.173365114574098.\n",
      "Step: 57500. Mean Reward: 4.252351062122822. Std of Reward: 10.409237539214383.\n",
      "Step: 60000. Mean Reward: 7.5489860976581085. Std of Reward: 12.068638278094548.\n",
      "Step: 62500. Mean Reward: 22.26612017956229. Std of Reward: 21.963125673052968.\n",
      "Step: 65000. Mean Reward: 12.522179383520996. Std of Reward: 18.95971610774098.\n",
      "Step: 67500. Mean Reward: 18.312063473562883. Std of Reward: 21.044290458758486.\n",
      "Step: 70000. Mean Reward: 13.802322136307295. Std of Reward: 18.875946908218573.\n",
      "Step: 72500. Mean Reward: 15.196734503267374. Std of Reward: 19.95125694534887.\n",
      "Step: 75000. Mean Reward: 7.449374556440626. Std of Reward: 12.086480660199484.\n",
      "Step: 77500. Mean Reward: 16.40384063926771. Std of Reward: 20.053259622473643.\n",
      "Step: 80000. Mean Reward: 9.028650388186367. Std of Reward: 12.350328420326093.\n",
      "Step: 82500. Mean Reward: 17.974286241782384. Std of Reward: 18.92246371029706.\n",
      "Step: 85000. Mean Reward: 17.100106111191955. Std of Reward: 19.15102914494966.\n",
      "Step: 87500. Mean Reward: 18.11570668483916. Std of Reward: 17.761357801318326.\n",
      "Step: 90000. Mean Reward: 14.226853701923805. Std of Reward: 17.981051323596795.\n",
      "Step: 92500. Mean Reward: 6.084898960698533. Std of Reward: 10.499375745039764.\n",
      "Step: 95000. Mean Reward: 8.166450157250098. Std of Reward: 15.412056859761108.\n",
      "Step: 97500. Mean Reward: 15.817677801596489. Std of Reward: 18.973786775023562.\n",
      "Step: 100000. Mean Reward: 18.118205794984664. Std of Reward: 22.959586111485546.\n",
      "Saved Model\n",
      "Step: 102500. Mean Reward: 26.82949076897446. Std of Reward: 25.658304661431885.\n",
      "Step: 105000. Mean Reward: 20.647898140370636. Std of Reward: 23.555943143398196.\n",
      "Step: 107500. Mean Reward: 16.71169527775047. Std of Reward: 19.10075160812829.\n",
      "Step: 110000. Mean Reward: 26.458325042242905. Std of Reward: 25.583864566868716.\n",
      "Step: 112500. Mean Reward: 11.567448715336008. Std of Reward: 16.393946307078743.\n",
      "Step: 115000. Mean Reward: 8.368952677780984. Std of Reward: 10.855581813763749.\n",
      "Step: 117500. Mean Reward: 14.448450873641887. Std of Reward: 16.810411194432568.\n",
      "Step: 120000. Mean Reward: 15.758809406825248. Std of Reward: 21.380606196743223.\n",
      "Step: 122500. Mean Reward: 7.978930412718746. Std of Reward: 13.701770896280003.\n",
      "Step: 125000. Mean Reward: 16.95780174236877. Std of Reward: 21.092090515699.\n",
      "Step: 127500. Mean Reward: 19.38097777966682. Std of Reward: 20.191446320015014.\n",
      "Step: 130000. Mean Reward: 17.134355932684844. Std of Reward: 18.993633495992437.\n",
      "Step: 132500. Mean Reward: 14.136190465740832. Std of Reward: 18.027816018527552.\n",
      "Step: 135000. Mean Reward: 13.373624909283457. Std of Reward: 16.546424177898164.\n",
      "Step: 137500. Mean Reward: 22.353645034199793. Std of Reward: 23.838953445930922.\n",
      "Step: 140000. Mean Reward: 29.583055694152065. Std of Reward: 24.235914220104757.\n",
      "Step: 142500. Mean Reward: 19.13649944090508. Std of Reward: 20.43340006128007.\n",
      "Step: 145000. Mean Reward: 26.06353433387445. Std of Reward: 25.621392005942546.\n",
      "Step: 147500. Mean Reward: 29.15800549356283. Std of Reward: 24.836304553171978.\n",
      "Step: 150000. Mean Reward: 23.66492502718975. Std of Reward: 25.396878650975943.\n",
      "Saved Model\n",
      "Step: 152500. Mean Reward: 34.177399789268016. Std of Reward: 24.516599682369097.\n",
      "Step: 155000. Mean Reward: 32.93941428274161. Std of Reward: 23.440503895052217.\n",
      "Step: 157500. Mean Reward: 26.188466934717834. Std of Reward: 24.60686662064345.\n",
      "Step: 160000. Mean Reward: 46.785443104089616. Std of Reward: 25.879768272481602.\n",
      "Step: 162500. Mean Reward: 32.70468522950644. Std of Reward: 30.188965728286036.\n",
      "Step: 165000. Mean Reward: 24.966981085555695. Std of Reward: 26.385469554860993.\n",
      "Step: 167500. Mean Reward: 30.07526580210024. Std of Reward: 26.09217102577451.\n",
      "Step: 170000. Mean Reward: 31.081642918544397. Std of Reward: 23.510473409446643.\n",
      "Step: 172500. Mean Reward: 28.852141892245587. Std of Reward: 22.81555810124266.\n",
      "Step: 175000. Mean Reward: 34.53058624490817. Std of Reward: 23.728421519624376.\n",
      "Step: 177500. Mean Reward: 35.394258068465156. Std of Reward: 24.822740378625817.\n",
      "Step: 180000. Mean Reward: 37.51082708365219. Std of Reward: 27.313077995189833.\n",
      "Step: 182500. Mean Reward: 49.628960509315. Std of Reward: 20.04582298616379.\n",
      "Step: 185000. Mean Reward: 30.65411945001026. Std of Reward: 24.101973926923943.\n",
      "Step: 187500. Mean Reward: 39.29752180000893. Std of Reward: 22.2036753219688.\n",
      "Step: 190000. Mean Reward: 31.63026497709856. Std of Reward: 23.566544742806364.\n",
      "Step: 192500. Mean Reward: 14.65255969099707. Std of Reward: 20.036839193555174.\n",
      "Step: 195000. Mean Reward: 27.795072292705253. Std of Reward: 21.434962762235504.\n",
      "Step: 197500. Mean Reward: 38.27276485844998. Std of Reward: 22.820693023794803.\n",
      "Step: 200000. Mean Reward: 45.24049767846307. Std of Reward: 24.75186411595827.\n",
      "Saved Model\n",
      "Step: 202500. Mean Reward: 19.197249652222897. Std of Reward: 22.076890425607793.\n",
      "Step: 205000. Mean Reward: 31.50989575475575. Std of Reward: 23.45592109301186.\n",
      "Step: 207500. Mean Reward: 42.02351975238934. Std of Reward: 27.46050037760069.\n",
      "Step: 210000. Mean Reward: 27.8438192280334. Std of Reward: 23.977462902196695.\n",
      "Step: 212500. Mean Reward: 20.88777282138015. Std of Reward: 22.5083181341427.\n",
      "Step: 215000. Mean Reward: 16.827180207494365. Std of Reward: 20.126036203731886.\n",
      "Step: 217500. Mean Reward: 26.77419134315898. Std of Reward: 23.41832272817476.\n",
      "Step: 220000. Mean Reward: 16.991195783986875. Std of Reward: 18.1168071476705.\n",
      "Step: 222500. Mean Reward: 17.67824753091463. Std of Reward: 19.663714212620103.\n",
      "Step: 225000. Mean Reward: 33.75156999226639. Std of Reward: 25.328108217796654.\n",
      "Step: 227500. Mean Reward: 28.785071978796186. Std of Reward: 25.79695944498642.\n",
      "Step: 230000. Mean Reward: 30.829709114893618. Std of Reward: 27.18855713884209.\n",
      "Step: 232500. Mean Reward: 22.574091093681695. Std of Reward: 23.705334491758897.\n",
      "Step: 235000. Mean Reward: 45.64738769214274. Std of Reward: 27.32145969110835.\n",
      "Step: 237500. Mean Reward: 34.670698729576266. Std of Reward: 27.686460786870995.\n",
      "Step: 240000. Mean Reward: 41.587234146074316. Std of Reward: 26.591558311127987.\n",
      "Step: 242500. Mean Reward: 31.610979915743236. Std of Reward: 27.189277869519525.\n",
      "Step: 245000. Mean Reward: 39.89166740571598. Std of Reward: 26.848459320149267.\n",
      "Step: 247500. Mean Reward: 39.1706777688277. Std of Reward: 24.78804462132218.\n",
      "Step: 250000. Mean Reward: 30.86262873816022. Std of Reward: 22.906965107780756.\n",
      "Saved Model\n",
      "Step: 252500. Mean Reward: 36.076351027947396. Std of Reward: 22.017147292184514.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 255000. Mean Reward: 42.757092022872634. Std of Reward: 22.525882094252115.\n",
      "Step: 257500. Mean Reward: 24.602719821477336. Std of Reward: 23.462478036887514.\n",
      "Step: 260000. Mean Reward: 31.103824573009728. Std of Reward: 25.233491881853155.\n",
      "Step: 262500. Mean Reward: 24.585503683714904. Std of Reward: 23.874756780498203.\n",
      "Step: 265000. Mean Reward: 24.536298507201867. Std of Reward: 22.91321139685587.\n",
      "Step: 267500. Mean Reward: 26.925739320278364. Std of Reward: 24.24162650151922.\n",
      "Step: 270000. Mean Reward: 38.04735637427107. Std of Reward: 27.720049653307434.\n",
      "Step: 272500. Mean Reward: 39.46044291233467. Std of Reward: 22.727251046800482.\n",
      "Step: 275000. Mean Reward: 27.5411726494002. Std of Reward: 24.97362692658726.\n",
      "Step: 277500. Mean Reward: 22.575632517846547. Std of Reward: 23.952243913945484.\n",
      "Step: 280000. Mean Reward: 27.489343817903855. Std of Reward: 23.177938306458262.\n",
      "Step: 282500. Mean Reward: 34.95797619739082. Std of Reward: 25.309746041298215.\n",
      "Step: 285000. Mean Reward: 51.94182023228105. Std of Reward: 22.34512643027846.\n",
      "Step: 287500. Mean Reward: 45.9648819665579. Std of Reward: 29.23205103805436.\n",
      "Step: 290000. Mean Reward: 51.70429450949146. Std of Reward: 25.64399164599182.\n",
      "Step: 292500. Mean Reward: 49.360016328240334. Std of Reward: 23.80535317799487.\n",
      "Step: 295000. Mean Reward: 40.42652200267029. Std of Reward: 26.674097242948786.\n",
      "Step: 297500. Mean Reward: 37.01050786657786. Std of Reward: 26.576961860205124.\n",
      "Step: 300000. Mean Reward: 36.776464036027214. Std of Reward: 27.342925476191535.\n",
      "Saved Model\n",
      "Step: 302500. Mean Reward: 41.3586034901021. Std of Reward: 29.868538511928406.\n",
      "Step: 305000. Mean Reward: 43.74329858814548. Std of Reward: 28.430782974214743.\n",
      "Step: 307500. Mean Reward: 45.746909672049256. Std of Reward: 26.64638838901767.\n",
      "Step: 310000. Mean Reward: 57.34465297607537. Std of Reward: 21.147731056571228.\n",
      "Step: 312500. Mean Reward: 51.10058022102115. Std of Reward: 26.49609005522271.\n",
      "Step: 315000. Mean Reward: 62.60775979643094. Std of Reward: 16.70847922173124.\n",
      "Step: 317500. Mean Reward: 55.68161923110343. Std of Reward: 23.1252826216517.\n",
      "Step: 320000. Mean Reward: 47.36689313071516. Std of Reward: 24.438039259177447.\n",
      "Step: 322500. Mean Reward: 46.54983962169818. Std of Reward: 29.60582247555671.\n",
      "Step: 325000. Mean Reward: 49.13705395087466. Std of Reward: 27.882922952698245.\n",
      "Step: 327500. Mean Reward: 41.979419969835774. Std of Reward: 29.03401629565962.\n",
      "Step: 330000. Mean Reward: 51.5136790898115. Std of Reward: 24.38779064645217.\n",
      "Step: 332500. Mean Reward: 33.1068238189408. Std of Reward: 28.274078717585613.\n",
      "Step: 335000. Mean Reward: 52.81863909682122. Std of Reward: 22.707994067911056.\n",
      "Step: 337500. Mean Reward: 48.084377505314855. Std of Reward: 28.82704896081553.\n",
      "Step: 340000. Mean Reward: 46.634626524535825. Std of Reward: 28.968592663421.\n",
      "Step: 342500. Mean Reward: 35.40793589480362. Std of Reward: 29.305167078921496.\n",
      "Step: 345000. Mean Reward: 58.21181646567836. Std of Reward: 21.639664286144505.\n",
      "Step: 347500. Mean Reward: 42.696114973498496. Std of Reward: 25.68128560685113.\n",
      "Step: 350000. Mean Reward: 57.67482611565612. Std of Reward: 24.352326601797387.\n",
      "Saved Model\n",
      "Step: 352500. Mean Reward: 58.510563790780026. Std of Reward: 22.96197163505465.\n",
      "Step: 355000. Mean Reward: 65.27294628662456. Std of Reward: 14.66596592658552.\n",
      "Step: 357500. Mean Reward: 49.47528283214039. Std of Reward: 28.933494337539855.\n",
      "Step: 360000. Mean Reward: 47.318147745207455. Std of Reward: 29.89922688243963.\n",
      "Step: 362500. Mean Reward: 55.53303973178776. Std of Reward: 24.803349642911265.\n",
      "Step: 365000. Mean Reward: 64.76138999090277. Std of Reward: 15.976076998624137.\n",
      "Step: 367500. Mean Reward: 51.42236847306695. Std of Reward: 29.34360154188797.\n",
      "Step: 370000. Mean Reward: 58.14463536216723. Std of Reward: 21.93153565222944.\n",
      "Step: 372500. Mean Reward: 51.73827514427568. Std of Reward: 24.44696482959038.\n",
      "Step: 375000. Mean Reward: 45.109435122952604. Std of Reward: 25.863676950086933.\n",
      "Step: 377500. Mean Reward: 53.5268365268542. Std of Reward: 25.574864731385677.\n",
      "Step: 380000. Mean Reward: 44.7759844218044. Std of Reward: 25.81588507827325.\n",
      "Step: 382500. Mean Reward: 42.6229887235669. Std of Reward: 28.048290095809524.\n",
      "Step: 385000. Mean Reward: 62.643091513009516. Std of Reward: 16.20455978498185.\n",
      "Step: 387500. Mean Reward: 60.27981477385086. Std of Reward: 23.396626247519123.\n",
      "Step: 390000. Mean Reward: 52.141664760332624. Std of Reward: 27.05918713442454.\n",
      "Step: 392500. Mean Reward: 55.31873011804709. Std of Reward: 23.93051212794331.\n",
      "Step: 395000. Mean Reward: 54.579772915947636. Std of Reward: 24.45912465632731.\n",
      "Step: 397500. Mean Reward: 48.5602868324304. Std of Reward: 25.75979224333423.\n",
      "Step: 400000. Mean Reward: 46.84869408279613. Std of Reward: 30.44375881513066.\n",
      "Saved Model\n",
      "Step: 402500. Mean Reward: 56.01030846772872. Std of Reward: 26.42388774604387.\n",
      "Step: 405000. Mean Reward: 53.25390649174743. Std of Reward: 23.173522183363033.\n",
      "Step: 407500. Mean Reward: 49.89827327978563. Std of Reward: 25.223992381233124.\n",
      "Step: 410000. Mean Reward: 49.36000662628582. Std of Reward: 27.511349136555463.\n",
      "Step: 412500. Mean Reward: 37.7826666734494. Std of Reward: 27.729033378722036.\n",
      "Step: 415000. Mean Reward: 50.08782283896939. Std of Reward: 28.03833969602203.\n",
      "Step: 417500. Mean Reward: 50.74844865843348. Std of Reward: 27.592414345396605.\n",
      "Step: 420000. Mean Reward: 53.52974395203167. Std of Reward: 24.54208375975907.\n",
      "Step: 422500. Mean Reward: 43.419107821726776. Std of Reward: 30.101931325311597.\n",
      "Step: 425000. Mean Reward: 52.28517294556859. Std of Reward: 26.723863124852507.\n",
      "Step: 427500. Mean Reward: 46.25699052707263. Std of Reward: 26.78772477514737.\n",
      "Step: 430000. Mean Reward: 44.8352636620762. Std of Reward: 30.19081717005551.\n",
      "Step: 432500. Mean Reward: 48.67162016462465. Std of Reward: 26.361704800767086.\n",
      "Step: 435000. Mean Reward: 63.065917511862295. Std of Reward: 17.41434041636452.\n",
      "Step: 437500. Mean Reward: 53.74005741308202. Std of Reward: 23.468439310647213.\n",
      "Step: 440000. Mean Reward: 55.9341148274289. Std of Reward: 26.801715403527297.\n",
      "Step: 442500. Mean Reward: 65.80370306085206. Std of Reward: 15.33940077876198.\n",
      "Step: 445000. Mean Reward: 56.55775406849618. Std of Reward: 24.752143033830592.\n",
      "Step: 447500. Mean Reward: 41.89177619387789. Std of Reward: 29.260920132441033.\n",
      "Step: 450000. Mean Reward: 44.03587033140583. Std of Reward: 27.620088016030152.\n",
      "Saved Model\n",
      "Step: 452500. Mean Reward: 43.845092422858144. Std of Reward: 30.846692596832902.\n",
      "Step: 455000. Mean Reward: 42.811002817445605. Std of Reward: 29.457730373117613.\n",
      "Step: 457500. Mean Reward: 47.61168216419726. Std of Reward: 27.88490836041877.\n",
      "Step: 460000. Mean Reward: 49.04027696353104. Std of Reward: 27.359515597995333.\n",
      "Step: 462500. Mean Reward: 49.498227222836945. Std of Reward: 30.08027173284088.\n",
      "Step: 465000. Mean Reward: 56.78690366335383. Std of Reward: 24.960417387320163.\n",
      "Step: 467500. Mean Reward: 50.937727498089174. Std of Reward: 27.90892416899637.\n",
      "Step: 470000. Mean Reward: 49.523776540222705. Std of Reward: 28.99841557984901.\n",
      "Step: 472500. Mean Reward: 61.237743421335225. Std of Reward: 21.10450383948573.\n",
      "Step: 475000. Mean Reward: 63.298670636746394. Std of Reward: 18.209073501877434.\n",
      "Step: 477500. Mean Reward: 58.99283988346121. Std of Reward: 24.10888637720659.\n",
      "Step: 480000. Mean Reward: 47.64113116679702. Std of Reward: 29.31477249715283.\n",
      "Step: 482500. Mean Reward: 53.15279333643585. Std of Reward: 26.478349870479928.\n",
      "Step: 485000. Mean Reward: 47.218089259396265. Std of Reward: 31.976405382667416.\n",
      "Step: 487500. Mean Reward: 48.12145988868288. Std of Reward: 29.181501492528533.\n",
      "Step: 490000. Mean Reward: 58.23868318244684. Std of Reward: 24.619750166030094.\n",
      "Step: 492500. Mean Reward: 66.21649838536156. Std of Reward: 16.850470126710057.\n",
      "Step: 495000. Mean Reward: 49.41074739494316. Std of Reward: 29.02753333209255.\n",
      "Step: 497500. Mean Reward: 40.85870165851325. Std of Reward: 32.62187458986793.\n",
      "Step: 500000. Mean Reward: 54.454590297765385. Std of Reward: 26.78613000177598.\n",
      "Saved Model\n",
      "Saved Model\n",
      "INFO:tensorflow:Restoring parameters from ./models/ppo\\model-500001.cptk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/ppo\\model-500001.cptk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 9 variables.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 9 variables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 9 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "if curriculum_file == \"None\":\n",
    "    curriculum_file = None\n",
    "\n",
    "\n",
    "def get_progress():\n",
    "    if curriculum_file is not None:\n",
    "        if env._curriculum.measure_type == \"progress\":\n",
    "            return steps / max_steps\n",
    "        elif env._curriculum.measure_type == \"reward\":\n",
    "            return last_reward\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Create the Tensorflow model graph\n",
    "ppo_model = create_agent_model(env, lr=learning_rate,\n",
    "                               h_size=hidden_units, epsilon=epsilon,\n",
    "                               beta=beta, max_step=max_steps, \n",
    "                               normalize=normalize, num_layers=num_layers)\n",
    "\n",
    "is_continuous = (env.brains[brain_name].action_space_type == \"continuous\")\n",
    "use_observations = (env.brains[brain_name].number_observations > 0)\n",
    "use_states = (env.brains[brain_name].state_space_size > 0)\n",
    "\n",
    "model_path = './models/{}'.format(run_path)\n",
    "summary_path = './summaries/{}'.format(run_path)\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "\n",
    "if not os.path.exists(summary_path):\n",
    "    os.makedirs(summary_path)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Instantiate model parameters\n",
    "    if load_model:\n",
    "        print('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        sess.run(init)\n",
    "    steps, last_reward = sess.run([ppo_model.global_step, ppo_model.last_reward])    \n",
    "    summary_writer = tf.summary.FileWriter(summary_path)\n",
    "    info = env.reset(train_mode=train_model, progress=get_progress())[brain_name]\n",
    "    trainer = Trainer(ppo_model, sess, info, is_continuous, use_observations, use_states, train_model)\n",
    "    if train_model:\n",
    "        trainer.write_text(summary_writer, 'Hyperparameters', hyperparameter_dict, steps)\n",
    "    while steps <= max_steps:\n",
    "        if env.global_done:\n",
    "            info = env.reset(train_mode=train_model, progress=get_progress())[brain_name]\n",
    "        # Decide and take an action\n",
    "        new_info = trainer.take_action(info, env, brain_name, steps, normalize)\n",
    "        info = new_info\n",
    "        trainer.process_experiences(info, time_horizon, gamma, lambd)\n",
    "        if len(trainer.training_buffer['actions']) > buffer_size and train_model:\n",
    "            # Perform gradient descent with experience buffer\n",
    "            trainer.update_model(batch_size, num_epoch)\n",
    "        if steps % summary_freq == 0 and steps != 0 and train_model:\n",
    "            # Write training statistics to tensorboard.\n",
    "            trainer.write_summary(summary_writer, steps, env._curriculum.lesson_number)\n",
    "        if steps % save_freq == 0 and steps != 0 and train_model:\n",
    "            # Save Tensorflow model\n",
    "            save_model(sess, model_path=model_path, steps=steps, saver=saver)\n",
    "        steps += 1\n",
    "        sess.run(ppo_model.increment_step)\n",
    "        if len(trainer.stats['cumulative_reward']) > 0:\n",
    "            mean_reward = np.mean(trainer.stats['cumulative_reward'])\n",
    "            sess.run(ppo_model.update_reward, feed_dict={ppo_model.new_reward: mean_reward})\n",
    "            last_reward = sess.run(ppo_model.last_reward)\n",
    "    # Final save Tensorflow model\n",
    "    if steps != 0 and train_model:\n",
    "        save_model(sess, model_path=model_path, steps=steps, saver=saver)\n",
    "env.close()\n",
    "export_graph(model_path, env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the trained Tensorflow graph\n",
    "Once the model has been trained and saved, we can export it as a .bytes file which Unity can embed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_graph(model_path, env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
